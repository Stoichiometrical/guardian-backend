{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0K7ZNuKNwY-",
    "outputId": "b24cdfad-c38b-44ad-ea4e-f42d7aa21376"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Number of records\n",
    "num_records = 40000\n",
    "num_users = 10\n",
    "\n",
    "# Generate user_ids\n",
    "user_ids = [f'user_{i+1}' for i in range(num_users)]\n",
    "\n",
    "# Function to generate random timestamps\n",
    "def generate_random_timestamps(start, end, n):\n",
    "    start_u = start.timestamp()\n",
    "    end_u = end.timestamp()\n",
    "    return [datetime.fromtimestamp(random.uniform(start_u, end_u)) for _ in range(n)]\n",
    "\n",
    "# Generate data\n",
    "data = []\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "for _ in range(num_records):\n",
    "    user_id = random.choice(user_ids)\n",
    "    timestamp = random.choice(generate_random_timestamps(start_date, end_date, 1))\n",
    "    location_change = random.randint(0, 1)\n",
    "    call_duration = random.randint(0, 600)  # Call duration between 0 and 10 minutes\n",
    "    message_count = random.randint(0, 20)  # Number of messages between 0 and 20\n",
    "    anomaly = random.choices([0, 1], weights=[0.95, 0.05])[0]  # 5% of records are anomalies\n",
    "\n",
    "    data.append([user_id, timestamp, location_change, call_duration, message_count, anomaly])\n",
    "\n",
    "# Create DataFrame\n",
    "columns = ['user_id', 'timestamp', 'location_change', 'call_duration', 'message_count', 'anomaly']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('dummy_data.csv', index=False)\n",
    "\n",
    "print(\"Dummy dataset generated and saved to 'dummy_data.csv'.\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dummy_data.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Drop any rows with missing values (if any)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df[['user_id', 'timestamp', 'location_change', 'call_duration', 'message_count']]\n",
    "y = df['anomaly']\n"
   ],
   "metadata": {
    "id": "qix2ubjPOMIs"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Encode user_id\n",
    "label_encoder = LabelEncoder()\n",
    "X['user_id'] = label_encoder.fit_transform(X['user_id'])\n",
    "\n",
    "# Extract time-based features\n",
    "X['hour'] = X['timestamp'].dt.hour\n",
    "X['day_of_week'] = X['timestamp'].dt.dayofweek\n",
    "\n",
    "# Drop the original timestamp column\n",
    "X.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W845jZuwOcyP",
    "outputId": "5143903f-87f2-4221-c4ab-8e289f63eecf"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize the model\n",
    "model = IsolationForest(contamination=0.05)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_scaled)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "xaqtFubmOf31",
    "outputId": "d916580c-bed8-41c0-c6f5-5bcda363c76e"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Predict anomalies on the training data\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Convert predictions from -1 (anomaly) and 1 (normal) to 1 (anomaly) and 0 (normal)\n",
    "y_pred = [1 if p == -1 else 0 for p in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y, y_pred))\n",
    "print(confusion_matrix(y, y_pred))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc8jMtuPOj6O",
    "outputId": "3fb0fdd1-e390-4cec-9c22-2d594e0a8780"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dummy_data.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Drop any rows with missing values (if any)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df[['user_id', 'timestamp', 'location_change', 'call_duration', 'message_count']]\n",
    "y = df['anomaly']\n",
    "\n",
    "# Encode user_id\n",
    "label_encoder = LabelEncoder()\n",
    "X['user_id'] = label_encoder.fit_transform(X['user_id'])\n",
    "\n",
    "# Extract time-based features\n",
    "X['hour'] = X['timestamp'].dt.hour\n",
    "X['day_of_week'] = X['timestamp'].dt.dayofweek\n",
    "\n",
    "# Drop the original timestamp column\n",
    "X.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train)\n",
    "\n",
    "# Predict anomalies on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions from -1 (anomaly) and 1 (normal) to 1 (anomaly) and 0 (normal)\n",
    "y_pred = [1 if p == -1 else 0 for p in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores):.2f})\")\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_samples': ['auto', 0.5, 0.75],\n",
    "    'contamination': [0.01, 0.05, 0.1],\n",
    "    'max_features': [1.0, 0.5, 0.75],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(IsolationForest(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train)\n",
    "\n",
    "# Predict using the best model\n",
    "y_best_pred = best_model.predict(X_test)\n",
    "y_best_pred = [1 if p == -1 else 0 for p in y_best_pred]\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best Model Classification Report:\")\n",
    "print(classification_report(y_test, y_best_pred))\n",
    "print(\"Best Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_best_pred))\n",
    "best_accuracy = accuracy_score(y_test, y_best_pred)\n",
    "print(f\"Best Model Accuracy: {best_accuracy:.2f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2B6q-DBlQKf1",
    "outputId": "c1b0acac-ecc2-4c60-970a-6e2aa9f79a3b"
   },
   "execution_count": 6,
   "outputs": []
  }
 ]
}
